import regex as re
from .base import Tokenizer

# GPT-4 经典的正则分割模式
GPT4_SPLIT_PATTERN = r"""'(?i:[sdmtllve]|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

class MiniBPETokenizer(Tokenizer):
    def __init__(self):
        super().__init__()
        self.compiled_pattern = re.compile(GPT4_SPLIT_PATTERN)

    def _get_stats(self, ids, counts=None):
        """统计相邻对出现的频次"""
        counts = {} if counts is None else counts
        for i in range(len(ids) - 1):
            pair = (ids[i], ids[i+1])
            counts[pair] = counts.get(pair, 0) + 1
        return counts

    def _merge(self, ids, pair, idx):
        """将 ids 中所有的 pair 替换为新的 idx"""
        new_ids = []
        i = 0
        while i < len(ids):
            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:
                new_ids.append(idx)
                i += 2
            else:
                new_ids.append(ids[i])
                i += 1
        return new_ids

    def train(self, text, vocab_size, verbose=False):
        num_merges = vocab_size - 256
        # 将文本按正则切分，并转为基础字节
        text_chunks = re.findall(self.compiled_pattern, text)
        ids = [list(ch.encode("utf-8")) for ch in text_chunks]

        merges = {} # (int, int) -> int
        vocab = {i: bytes([i]) for i in range(256)} 
        
        for i in range(num_merges):
            # 统计所有 chunk 中的对频次
            stats = {}
            for chunk_ids in ids:
                self._get_stats(chunk_ids, stats)
            
            if not stats: break
            
            # 找到出现次数最多的对
            pair = max(stats, key=stats.get)
            idx = 256 + i
            
            # 更新所有的 chunk
            ids = [self._merge(chunk_ids, pair, idx) for chunk_ids in ids]
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]
            
            if verbose:
                print(f"merge {i+1}/{num_merges}: {pair} -> {idx}")

        self.merges = merges
        self.vocab = vocab

    def encode(self, text):
        text_chunks = re.findall(self.compiled_pattern, text)
        all_ids = []
        for chunk in text_chunks:
            chunk_ids = list(chunk.encode("utf-8"))
            while len(chunk_ids) >= 2:
                stats = self._get_stats(chunk_ids)
                # 寻找当前 chunk 中在 merges 表里最先发生的合并
                pair = min(stats.keys(), key=lambda p: self.merges.get(p, float("inf")))
                if pair not in self.merges:
                    break
                idx = self.merges[pair]
                chunk_ids = self._merge(chunk_ids, pair, idx)
            all_ids.extend(chunk_ids)
        return all_ids

    def decode(self, ids):
        part_bytes = []
        for idx in ids:
            if idx in self.vocab:
                part_bytes.append(self.vocab[idx])
        return b"".join(part_bytes).decode("utf-8", errors="replace")
